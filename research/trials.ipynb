{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "root_dir = Path.cwd().parent\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Record Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "from fluent_flow import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(duration=5, fs=44100):\n",
    "    \"\"\"Record audio for a specified duration.\"\"\"\n",
    "    logger.info(\"Recording will start in 3 seconds...\")\n",
    "    sd.sleep(3000)\n",
    "    logger.info(\"Recording started...\")\n",
    "    \n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    \n",
    "    logger.info(\"Recording finished.\")\n",
    "    return recording.flatten()\n",
    "\n",
    "def save_audio(data, filename=\"output.wav\", fs=44100):\n",
    "    sf.write(filename, data, fs)\n",
    "    logger.info(f\"Audio saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data = record_audio(duration=10)  # Adjust duration as needed\n",
    "save_audio(audio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Speech to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import wave\n",
    "\n",
    "root_dir = Path.cwd().parent\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "from fluent_flow import logger\n",
    "\n",
    "# Make sure you have Vosk installed: pip install vosk\n",
    "from vosk import Model, KaldiRecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def speech_to_text(audio_file, model_path):\n",
    "    \"\"\"\n",
    "    Convert speech to text using Vosk.\n",
    "    \n",
    "    :param audio_file: Path to the audio file\n",
    "    :param model_path: Path to the Vosk model\n",
    "    :return: Transcribed text\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting speech-to-text conversion for {audio_file}\")\n",
    "    \n",
    "    # Check if model path exists\n",
    "    if not Path(model_path).exists():\n",
    "        logger.error(f\"Model path does not exist: {model_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Load Vosk model\n",
    "        model = Model(model_path)\n",
    "        \n",
    "        # Open the audio file\n",
    "        wf = wave.open(str(audio_file), \"rb\")\n",
    "        \n",
    "        # Check if the audio format is compatible\n",
    "        if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != \"NONE\":\n",
    "            logger.error(\"Audio file must be WAV format mono PCM.\")\n",
    "            return None\n",
    "        \n",
    "        # Create recognizer\n",
    "        rec = KaldiRecognizer(model, wf.getframerate())\n",
    "        rec.SetWords(True)\n",
    "        \n",
    "        # Process audio file\n",
    "        results = []\n",
    "        while True:\n",
    "            data = wf.readframes(4000)\n",
    "            if len(data) == 0:\n",
    "                break\n",
    "            if rec.AcceptWaveform(data):\n",
    "                part_result = json.loads(rec.Result())\n",
    "                results.append(part_result)\n",
    "        \n",
    "        part_result = json.loads(rec.FinalResult())\n",
    "        results.append(part_result)\n",
    "        \n",
    "        # Extract text from results\n",
    "        text = \" \".join([r['text'] for r in results if 'text' in r])\n",
    "        \n",
    "        logger.info(\"Speech-to-text conversion completed\")\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in speech-to-text conversion: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio_file = \"output.wav\"  # This should be the file saved in step 1\n",
    "model_path = \"vosk-model-small-de-015\"  # Replace with the path to your Vosk model\n",
    "\n",
    "transcribed_text = speech_to_text(audio_file, model_path)\n",
    "\n",
    "if transcribed_text:\n",
    "    logger.info(f\"Transcribed text: {transcribed_text}\")\n",
    "else:\n",
    "    logger.error(\"Failed to transcribe audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from fluent_flow import logger\n",
    "\n",
    "def speech_to_text_with_whisper(audio_file_path, model=\"whisper-1\"):\n",
    "    \"\"\"\n",
    "    Convert speech to text using OpenAI Whisper.\n",
    "    \n",
    "    :param audio_file_path: Path to the audio file\n",
    "    :param model: Whisper model to use (default is \"whisper-1\")\n",
    "    :return: Transcribed text\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting speech-to-text conversion for {audio_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Open the audio file in binary mode\n",
    "        with open(audio_file_path, \"rb\") as audio_file:\n",
    "            # Transcribe using OpenAI Whisper\n",
    "            response = openai.Audio.transcriptions.create(\n",
    "                model=model,\n",
    "                file=audio_file,\n",
    "                response_format=\"text\"  # Options: \"text\", \"json\", \"srt\", \"verbose_json\", \"vtt\"\n",
    "            )\n",
    "        \n",
    "        transcribed_text = response['text']\n",
    "        logger.info(\"Speech-to-text conversion completed\")\n",
    "        return transcribed_text\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in speech-to-text conversion: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "audio_file = \"output.wav\" # Path to your WAV audio file\n",
    "transcribed_text = speech_to_text_with_whisper(audio_file)\n",
    "\n",
    "if transcribed_text:\n",
    "    logger.info(f\"Transcribed text: {transcribed_text}\")\n",
    "else:\n",
    "    logger.error(\"Failed to transcribe audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: Process Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_language_model():\n",
    "    \"\"\"Initialize LangChain with OpenAI as the language model.\"\"\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        logger.error(\"API key is not set in the environment variables.\")\n",
    "        return None\n",
    "    \n",
    "    llm = OpenAI(api_key=api_key, model=\"gpt-3.5-turbo-instruct\")\n",
    "    prompt_template = generate_prompt()\n",
    "    llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    logger.info(\"LLMChain initialized with OpenAI.\")\n",
    "    return llm_chain\n",
    "\n",
    "def generate_prompt():\n",
    "    \"\"\"Define the prompt template for the language model.\"\"\"\n",
    "    template = \"\"\"\n",
    "    Du bist ein hilfreicher Deutschlehrer. Deine Aufgabe ist es, die Eingaben des Benutzers auf Deutsch kurz und prägnant zu korrigieren und eine einfache Erklärung für die Korrektur zu geben. Halte die Konversation lebendig, indem du relevante Kommentare oder Vorschläge machst.\n",
    "\n",
    "    Hier ist ein Beispiel, wie du antworten solltest:\n",
    "    Mensch: Ich habe heute ein neues Buch gekauft.\n",
    "    KI: Korrektur: \"Ich habe heute ein neues Buch gekauft.\" (I bought a new book today.) \n",
    "    Erklärung: Der Satz ist korrekt! Bücher sind eine großartige Möglichkeit, neue Ideen zu entdecken. Welches Buch hast du gekauft?\n",
    "\n",
    "    Aktuelles Gespräch:\n",
    "    {chat_history}\n",
    "    Mensch: {input_text}\n",
    "    KI: Korrektur und Kommentar:\n",
    "    \"\"\"\n",
    "    return PromptTemplate(template=template, input_variables=[\"chat_history\", \"input_text\"])\n",
    "\n",
    "def process_text(llm_chain, input_text, chat_history):\n",
    "    \"\"\"Process the input text using the configured LLMChain.\"\"\"\n",
    "    logger.info(\"Processing text with LLMChain...\")\n",
    "    try:\n",
    "        # Prepare input variables\n",
    "        input_variables = {\n",
    "            \"chat_history\": chat_history,\n",
    "            \"input_text\": input_text\n",
    "        }\n",
    "        \n",
    "        # Debugging: Log the input variables\n",
    "        logger.info(f\"Input Variables: {input_variables}\")\n",
    "\n",
    "         # Generate the prompt using the PromptTemplate\n",
    "        prompt = llm_chain.prompt.format(**input_variables)\n",
    "        \n",
    "        # Debugging: Log the generated prompt\n",
    "        logger.info(f\"Generated Prompt: {prompt}\")\n",
    "\n",
    "        response = llm_chain.run(**input_variables)\n",
    "        logger.info(\"Text processing completed\")\n",
    "        return response.strip()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in text processing: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "def update_chat_history(chat_history, new_human_text, new_ai_text, max_turns=3):\n",
    "    \"\"\"Update chat history to keep only the last few exchanges.\"\"\"\n",
    "    chat_history += f\"Human: {new_human_text}\\nAI: {new_ai_text}\\n\"\n",
    "    # Split the chat history into turns\n",
    "    turns = chat_history.strip().split('\\n')\n",
    "    # Keep only the last `max_turns` exchanges\n",
    "    if len(turns) > max_turns * 2:\n",
    "        turns = turns[-max_turns * 2:]\n",
    "    return '\\n'.join(turns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = initialize_language_model()\n",
    "    \n",
    "if llm_chain:\n",
    "    transcribed_text = \"Moin! Wie geht es dir?\"\n",
    "    chat_history = \"\"\n",
    "    \n",
    "    processed_text = process_text(llm_chain, transcribed_text, chat_history)\n",
    "    if processed_text:\n",
    "        logger.info(f\"Processed text: {processed_text}\")\n",
    "        chat_history = update_chat_history(chat_history, transcribed_text, processed_text)\n",
    "    else:\n",
    "        logger.error(\"Failed to process text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribed_text = \"Auch gut, danke! Ja ich lebe ich Hamburg seit ne weile und 'Moin' ist mir beigrbracht\"\n",
    "processed_text = process_text(llm_chain, transcribed_text, chat_history)\n",
    "if processed_text:\n",
    "    logger.info(f\"Processed text: {processed_text}\")\n",
    "    chat_history = update_chat_history(chat_history, transcribed_text, processed_text)\n",
    "else:\n",
    "    logger.error(\"Failed to process text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: Text to Speech\n",
    "5: Play Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def text_to_speech(text, language='de', mp3_filename='output.mp3', wav_filename='output.wav'):\n",
    "    \"\"\"Convert text to speech in German and save as a WAV file.\"\"\"\n",
    "    try:\n",
    "        # Convert text to speech and save as MP3\n",
    "        tts = gTTS(text=text, lang=language)\n",
    "        tts.save(mp3_filename)\n",
    "        logger.info(f\"Text-to-speech conversion completed and saved to {mp3_filename}\")\n",
    "        \n",
    "        # Convert MP3 to WAV\n",
    "        audio = AudioSegment.from_mp3(mp3_filename)\n",
    "        audio.export(wav_filename, format='wav')\n",
    "        logger.info(f\"Audio converted to WAV and saved to {wav_filename}\")\n",
    "        \n",
    "        # Optionally, remove the MP3 file if you only want the WAV\n",
    "        os.remove(mp3_filename)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in text-to-speech conversion: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "def play_audio(filename='output.wav'):\n",
    "    \"\"\"Play the audio file.\"\"\"\n",
    "    try:\n",
    "        audio = AudioSegment.from_wav(filename)\n",
    "        play(audio)\n",
    "        logger.info(f\"Playing audio file {filename}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error playing audio file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After processing text\n",
    "transcribed_text = \"Also hamburg, bremen. Ich war auch inn Sylt\"\n",
    "processed_text = process_text(llm_chain, transcribed_text, chat_history)\n",
    "if processed_text:\n",
    "    logger.info(f\"Processed text: {processed_text}\")\n",
    "    chat_history = update_chat_history(chat_history, transcribed_text, processed_text)\n",
    "    \n",
    "    # Convert processed text to speech in German and save as WAV\n",
    "    text_to_speech(processed_text, wav_filename='response.wav')\n",
    "    \n",
    "    # Play the generated speech\n",
    "    play_audio('response.wav')\n",
    "else:\n",
    "    logger.error(\"Failed to process text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "import json\n",
    "from vosk import Model, KaldiRecognizer\n",
    "import openai\n",
    "from fluent_flow import logger\n",
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import queue\n",
    "import sys\n",
    "\n",
    "# Function to record audio\n",
    "def record_audio(filename, duration=10, samplerate=16000):\n",
    "    q = queue.Queue()\n",
    "\n",
    "    def callback(indata, frames, time, status):\n",
    "        if status:\n",
    "            print(status, file=sys.stderr)\n",
    "        q.put(indata.copy())\n",
    "\n",
    "    with sd.InputStream(samplerate=samplerate, channels=1, callback=callback):\n",
    "        print(f\"Recording for {duration} seconds...\")\n",
    "        with wave.open(filename, 'wb') as wf:\n",
    "            wf.setnchannels(1)\n",
    "            wf.setsampwidth(2)\n",
    "            wf.setframerate(samplerate)\n",
    "            for _ in range(int(samplerate / 1024 * duration)):\n",
    "                wf.writeframes(q.get())\n",
    "\n",
    "def start_conversation(vosk_model_path, openai_model=\"whisper-1\"):\n",
    "    \"\"\"\n",
    "    Start a conversation using speech-to-text, language model processing, and text-to-speech.\n",
    "    \n",
    "    :param vosk_model_path: Path to the Vosk model for speech recognition\n",
    "    :param openai_model: OpenAI Whisper model to use for speech-to-text\n",
    "    \"\"\"\n",
    "    # Initialize the language model\n",
    "    llm_chain = initialize_language_model()\n",
    "    if llm_chain is None:\n",
    "        logger.error(\"Failed to initialize language model.\")\n",
    "        return\n",
    "\n",
    "    chat_history = \"\"\n",
    "    \n",
    "    while True:\n",
    "        # Record audio input from the user\n",
    "        audio_file_path = \"user_input.wav\"\n",
    "        record_audio(audio_file_path, duration=5)\n",
    "\n",
    "        # Convert speech to text\n",
    "        transcribed_text = speech_to_text(audio_file_path, vosk_model_path)\n",
    "        if not transcribed_text:\n",
    "            logger.error(\"Failed to transcribe audio.\")\n",
    "            continue\n",
    "\n",
    "        # Process the text with the language model\n",
    "        ai_response = process_text(llm_chain, transcribed_text, chat_history)\n",
    "        if not ai_response:\n",
    "            logger.error(\"Failed to process text.\")\n",
    "            continue\n",
    "\n",
    "        # Update chat history\n",
    "        chat_history = update_chat_history(chat_history, transcribed_text, ai_response)\n",
    "\n",
    "        # Convert AI response to speech\n",
    "        text_to_speech(ai_response)\n",
    "\n",
    "        # Log the conversation\n",
    "        logger.info(f\"Human: {transcribed_text}\")\n",
    "        logger.info(f\"AI: {ai_response}\")\n",
    "\n",
    "        # Break condition (optional)\n",
    "        if \"exit\" in transcribed_text.lower():\n",
    "            logger.info(\"Exiting conversation.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-21 18:06:57,133: INFO: 3483679649: LLMChain initialized with OpenAI.]\n",
      "Recording for 5 seconds...\n",
      "[2024-08-21 18:06:59,926: INFO: 358404966: Starting speech-to-text conversion for user_input.wav]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from vosk-model-small-de-015/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from vosk-model-small-de-015/graph/HCLr.fst vosk-model-small-de-015/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo vosk-model-small-de-015/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-21 18:07:00,846: INFO: 358404966: Speech-to-text conversion completed]\n",
      "[2024-08-21 18:07:00,923: ERROR: 1227383979: Failed to transcribe audio.]\n",
      "Recording for 5 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Model.__del__ at 0x7faca890f790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/isma/Documents/Portfolio/fluent-flow/.venv/lib/python3.9/site-packages/vosk/__init__.py\", line 60, in __del__\n",
      "    _c.vosk_model_free(self._handle)\n",
      "AttributeError: 'Model' object has no attribute '_handle'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m vosk_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvosk-model-small-de-015\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mstart_conversation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvosk_model_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 49\u001b[0m, in \u001b[0;36mstart_conversation\u001b[0;34m(vosk_model_path, openai_model)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Record audio input from the user\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     audio_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_input.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mrecord_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Convert speech to text\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     transcribed_text \u001b[38;5;241m=\u001b[39m speech_to_text(audio_file_path, vosk_model_path)\n",
      "Cell \u001b[0;32mIn[27], line 29\u001b[0m, in \u001b[0;36mrecord_audio\u001b[0;34m(filename, duration, samplerate)\u001b[0m\n\u001b[1;32m     27\u001b[0m wf\u001b[38;5;241m.\u001b[39msetframerate(samplerate)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(samplerate \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m duration)):\n\u001b[0;32m---> 29\u001b[0m     wf\u001b[38;5;241m.\u001b[39mwriteframes(\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vosk_model_path = \"vosk-model-small-de-015\"\n",
    "start_conversation(vosk_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
