{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "root_dir = Path.cwd().parent\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Record Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "from src.fluent_flow import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(duration=5, fs=44100):\n",
    "    \"\"\"Record audio for a specified duration.\"\"\"\n",
    "    logger.info(\"Recording will start in 3 seconds...\")\n",
    "    sd.sleep(3000)\n",
    "    logger.info(\"Recording started...\")\n",
    "    \n",
    "    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    \n",
    "    logger.info(\"Recording finished.\")\n",
    "    return recording.flatten()\n",
    "\n",
    "def save_audio(data, filename=\"output.wav\", fs=44100):\n",
    "    sf.write(filename, data, fs)\n",
    "    logger.info(f\"Audio saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-08 10:42:38,353: INFO: 3822744899: Recording will start in 3 seconds...]\n",
      "[2024-08-08 10:42:41,356: INFO: 3822744899: Recording started...]\n",
      "[2024-08-08 10:42:51,684: INFO: 3822744899: Recording finished.]\n",
      "[2024-08-08 10:42:51,692: INFO: 3822744899: Audio saved to output.wav]\n"
     ]
    }
   ],
   "source": [
    "audio_data = record_audio(duration=10)  # Adjust duration as needed\n",
    "save_audio(audio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Speech to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import wave\n",
    "\n",
    "root_dir = Path.cwd().parent\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "from src.fluent_flow import logger\n",
    "\n",
    "# Make sure you have Vosk installed: pip install vosk\n",
    "from vosk import Model, KaldiRecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def speech_to_text(audio_file, model_path):\n",
    "    \"\"\"\n",
    "    Convert speech to text using Vosk.\n",
    "    \n",
    "    :param audio_file: Path to the audio file\n",
    "    :param model_path: Path to the Vosk model\n",
    "    :return: Transcribed text\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting speech-to-text conversion for {audio_file}\")\n",
    "    \n",
    "    # Check if model path exists\n",
    "    if not Path(model_path).exists():\n",
    "        logger.error(f\"Model path does not exist: {model_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Load Vosk model\n",
    "        model = Model(model_path)\n",
    "        \n",
    "        # Open the audio file\n",
    "        wf = wave.open(str(audio_file), \"rb\")\n",
    "        \n",
    "        # Check if the audio format is compatible\n",
    "        if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != \"NONE\":\n",
    "            logger.error(\"Audio file must be WAV format mono PCM.\")\n",
    "            return None\n",
    "        \n",
    "        # Create recognizer\n",
    "        rec = KaldiRecognizer(model, wf.getframerate())\n",
    "        rec.SetWords(True)\n",
    "        \n",
    "        # Process audio file\n",
    "        results = []\n",
    "        while True:\n",
    "            data = wf.readframes(4000)\n",
    "            if len(data) == 0:\n",
    "                break\n",
    "            if rec.AcceptWaveform(data):\n",
    "                part_result = json.loads(rec.Result())\n",
    "                results.append(part_result)\n",
    "        \n",
    "        part_result = json.loads(rec.FinalResult())\n",
    "        results.append(part_result)\n",
    "        \n",
    "        # Extract text from results\n",
    "        text = \" \".join([r['text'] for r in results if 'text' in r])\n",
    "        \n",
    "        logger.info(\"Speech-to-text conversion completed\")\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in speech-to-text conversion: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-08 10:44:31,667: INFO: 358404966: Starting speech-to-text conversion for output.wav]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from vosk-model-small-de-015/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from vosk-model-small-de-015/graph/HCLr.fst vosk-model-small-de-015/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo vosk-model-small-de-015/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-08 10:44:33,803: INFO: 358404966: Speech-to-text conversion completed]\n",
      "[2024-08-08 10:44:33,922: INFO: 2640530983: Transcribed text: er mehr geliebte den bus aber an diesem morgen zögerte sich in nein zu klettern als morgen nochmal zum haben]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "audio_file = \"output.wav\"  # This should be the file saved in step 1\n",
    "model_path = \"vosk-model-small-de-015\"  # Replace with the path to your Vosk model\n",
    "\n",
    "transcribed_text = speech_to_text(audio_file, model_path)\n",
    "\n",
    "if transcribed_text:\n",
    "    logger.info(f\"Transcribed text: {transcribed_text}\")\n",
    "else:\n",
    "    logger.error(\"Failed to transcribe audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: Process Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "root_dir = Path.cwd().parent\n",
    "\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "from src.fluent_flow import logger\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM,  AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_language_model():\n",
    "    \"\"\"Initialize the lightweight language model and memory.\"\"\"\n",
    "    # Initialize the Hugging Face model\n",
    "    model_name = \"EleutherAI/gpt-neo-1.3B\"  # or \"EleutherAI/gpt-j-6B\" for a larger model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # Use GPU if available\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    \n",
    "    model_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # Define the prompt template\n",
    "    template = \"\"\"\n",
    "    Du bist ein hilfreicher Deutschlehrer. Deine Aufgabe ist es, ein Gespräch auf Deutsch zu führen, \n",
    "    Fehler im Input des Benutzers zu korrigieren und kurze Erklärungen für die Korrekturen zu geben.\n",
    "    Antworte immer auf Deutsch, aber füge englische Übersetzungen in Klammern für wichtige Phrasen oder Korrekturen hinzu.\n",
    "\n",
    "    Aktuelles Gespräch:\n",
    "    {chat_history}\n",
    "\n",
    "    Mensch: {human_input}\n",
    "    KI: Lass uns unser Gespräch auf Deutsch fortsetzen und eventuelle Fehler korrigieren:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"chat_history\", \"human_input\"])\n",
    "    \n",
    "    # Create the language model chain\n",
    "    return LLMChain(llm=HuggingFacePipeline(pipeline=model_pipeline), prompt=prompt)\n",
    "\n",
    "def process_text(llm_chain, input_text, chat_history):\n",
    "    \"\"\"Process the input text using the language model chain.\"\"\"\n",
    "    logger.info(\"Processing text with language model...\")\n",
    "    try:\n",
    "        response = llm_chain.predict(human_input=input_text, chat_history=chat_history, max_new_tokens=256)\n",
    "        logger.info(\"Text processing completed\")\n",
    "        return response.strip()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in text processing: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_language_model():\n",
    "    \"\"\"Initialize the lightweight language model and memory.\"\"\"\n",
    "    # Initialize the Hugging Face model\n",
    "    model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # Use GPU if available\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    \n",
    "    model_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    return model_pipeline\n",
    "\n",
    "def process_text(model_pipeline, input_text, chat_history):\n",
    "    \"\"\"Process the input text using the language model pipeline.\"\"\"\n",
    "    logger.info(\"Processing text with language model...\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Du bist ein hilfreicher Deutschlehrer. Deine Aufgabe ist es, ein Gespräch auf Deutsch zu führen, \n",
    "    Fehler im Input des Benutzers zu korrigieren und kurze Erklärungen für die Korrekturen zu geben.\n",
    "    Antworte immer auf Deutsch, aber füge englische Übersetzungen in Klammern für wichtige Phrasen oder Korrekturen hinzu.\n",
    "\n",
    "    Aktuelles Gespräch:\n",
    "    {chat_history}\n",
    "\n",
    "    Mensch: {input_text}\n",
    "    KI: Lass uns unser Gespräch auf Deutsch fortsetzen und eventuelle Fehler korrigieren:\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model_pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens=256,  # Number of tokens to generate\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        text_response = response[0]['generated_text'].replace(prompt, \"\").strip()\n",
    "        logger.info(\"Text processing completed\")\n",
    "        return text_response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in text processing: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isma/Documents/Portfolio/fluent-flow/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-08 11:10:18,093: INFO: 1451791682: Processing text with language model...]\n",
      "[2024-08-08 11:12:10,972: INFO: 1451791682: Text processing completed]\n",
      "[2024-08-08 11:12:10,973: ERROR: 1152501599: Failed to process text]\n"
     ]
    }
   ],
   "source": [
    "llm_chain = initialize_language_model()\n",
    "    \n",
    "# Example transcribed text from step 2\n",
    "transcribed_text = \"Ich bin mude und ich mochte schlafen gehen\"\n",
    "\n",
    "# Initialize chat history\n",
    "chat_history = \"\"\n",
    "\n",
    "# Process the text\n",
    "processed_text = process_text(llm_chain, transcribed_text, chat_history)\n",
    "\n",
    "if processed_text:\n",
    "    logger.info(f\"Processed text: {processed_text}\")\n",
    "    # Update chat history\n",
    "    chat_history += f\"Human: {transcribed_text}\\nAI: {processed_text}\\n\"\n",
    "else:\n",
    "    logger.error(\"Failed to process text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length (number of tokens): 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isma/Documents/Portfolio/fluent-flow/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "###### Test layer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"  # or any other model you're using\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Your input text\n",
    "input_text = \"\"\"\n",
    "Du bist ein hilfreicher Deutschlehrer. Deine Aufgabe ist es, ein Gespräch auf Deutsch zu führen, \n",
    "Fehler im Input des Benutzers zu korrigieren und kurze Erklärungen für die Korrekturen zu geben.\n",
    "Antworte immer auf Deutsch, aber füge englische Übersetzungen in Klammern für wichtige Phrasen oder Korrekturen hinzu.\n",
    "\n",
    "Aktuelles Gespräch:\n",
    "Mensch: Ich bin müde und ich möchte schlafen gehen\n",
    "KI: Lass uns unser Gespräch auf Deutsch fortsetzen und eventuelle Fehler korrigieren:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Get the input_ids\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# Check the size of input_ids\n",
    "input_length = input_ids.size(1)  # The second dimension is the sequence length\n",
    "print(f\"Input length (number of tokens): {input_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: Text to Speech\n",
    "5: Play Audio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
